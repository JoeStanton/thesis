\documentclass{cshonours}

\usepackage{url}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage[style=alphabetic, sorting=nyt]{biblatex}
\addbibresource{bibliography.bib}

\title{6CCS3PRJ \\\vspace{0.5cm}
  A Pragmatic Infrastructure Modelling and Verification Framework}
\author{Joe Stanton\\\vspace{0.5cm}
  Student ID: 1020836
}

\begin{document}

\maketitle

\chapter*{Abstract}

%Trends:

%* Cloud Computing and Virtualization, LXC Containerization
%* Dynamic, Elastic, Auto-Scaling
%* Configuration Management
%* Immutable Infrastructure
%* Full Stack Monitoring
%* High Level Visualisation of Application Architecture

%Application architectures are more complex than ever,. The reliance on cloud computing offloads a large amount of complexity to the provider, but typically this is Infrastructure as a Service. It becomes harder to reason about the runtime behavior of the software.

The complexity of software and the scale at which services must operate is continuing to increase rapidly~\cite{SoftwareComplexity}.
To support this rapid growth, the supporting application infrastructure needed to keep these services running and providing a suitable quality of service must grow proportionally.

A typical stack for delivering scalable web based applications now consists of many interconnected parts, including database servers, application servers, caching layers, load balancers and network hardware.
This inevitable growth in both application and infrastructure complexity presents significant engineering challenges in the design of fault-tolerant and performant distributed systems.

While the problems of delivering software at scale are significantly reduced by the cloud computing movement, the inherent complexities of distributed systems with many components and therefore points of failure continually prove to be problematic. Infrastructure complexity is steadily increasing, and is a leading cause of downtime and unreliability. According to a recent Gartner survey, 73\% of IT executives say their infrastructure complexity is either `high' or `out of control'~\cite{Gartner2013}. Managing this complexity, and therefore being able to efficiently identify and fix issues is the key to providing the reliable and performant services we expect, over time.

In this report, we discuss and implement an infrastructure modelling and monitoring tool that can be used to help manage some of the inherent complexity in modern infrastructure. The emphasis of this investigation is on small to medium size software deployments, rather than enterprises that typically have the resources to tackle these problems.

\chapter*{Originality Avowal}
I verify that I am the sole author of this report, except where explicitly stated to the contrary. 

\textbf{Signed:} Joe Stanton 

\textbf{Date:} \today

% \chapter*{Acknowledgements}

\tableofcontents

\chapter{Introduction}

\section{Thesis}
\textit{It is practical and useful to develop an infrastructure modelling and monitoring tool to help address the inherent complexity in modern IT infrastructure.}
\section{Motivation}

After taking part in the delivery of software in the highly regulated insurance industry, I noted some problems with the reliability of the software delivered. These reliability issues were generally unrelated to the application itself, but predominantly caused by the growth in complexity of the underlying application infrastructure, which was provided and managed within the company's own datacenter as a result of regulatory constraints. As the application grew in complexity over time, the number of dependent services and components needed to keep it running became unmanageable by the company's own IT staff.

I discovered a real disparity between the knowledge application developers had of the specific requirements of their application (hardware specifications, dependencies on other services, likely points of failure) and the lack of awareness of this within the operations team. This phenomenon has been noted in other areas of the industry~\cite{DevOps} and has recently given rise to the term `DevOps' - an attempt to improve communication between the teams or build tools that help to do so.

Better collaboration and knowledge transfer between these teams often provides the following benefits:

\begin{description}
  \item [Proactive maintenance of the system and its dependencies]\hfill \\
    The effect of changes is well understood by all members of the team and problems can be foreseen ahead of time. The infrastructure can be modernised progressively.
  \item [Identification of weaknesses in the system architecture]\hfill \\
    e.g.\ single points of failure, overworked or repeatedly unreliable components can be identified, and discussion between the teams can lead to effective solutions. 
  \item [Reduced risk of change]\hfill \\
    Delivery of features and bug-fixes to deployed software can happen regularly and reliably through Continuous Delivery. 
  \item [Quick and effective root cause analysis]\hfill \\
    When issues do occur, they can be diagnosed and fixed quickly, and then prevented in the future.
\end{description}

\section{Solution Objectives}
%Embracing change is a key tenet of the cloud computing architecture.

The objective is to design and implement a tool that helps with the modelling of complex infrastructure within real-world systems. The application will provide application developers or operations staff the ability to model systems as a graph of nodes, which may have dependencies and fine grained tests used to verify their current state.

The tool will then execute this model as a means of system monitoring. It will verify the correctness of the live system continuously and warn of any divergence. It will also allow the attachment of meta-data and metrics to nodes. This model, meta-data and metrics will then serve as a useful diagnostic tool if the state of the system changes, e.g.\ a loss of connectivity between two nodes. This fine-grained and clear mechanism of communicating failure should reduce investigation and response times when issues arise, and provide the insight which prevents them from recurring.

This model can serve as `living documentation' that is used by Ops teams and Developers to help manage and transfer knowledge, documenting their implicit knowledge. It should help to predict the impact of changes. It should improve collaboration between the teams and provide deeper insight into issues with the performance or reliability of the system.

Languages and tools for the modelling of software architecture and infrastructure do exist, however they lack the simplicity and pragmatism in their implementation to see widespread adoption in industry~\cite{ModellingAdoption}. I propose a simple, pragmatic tool that may come with some modelling limitations, but will be quick and easy to set up and begin reaping the benefits.

Given its role as the centralised location of knowledge for the system, this tool could become a platform to solve other distributed systems issues, such as service discovery etc.\ but these problems are not part of the initial scope.

\section{Software Platform and Target OS}

The system will be developed in multiple parts and distributed across hosts, where an appropriate choice of programming language will be used to suit the task. The application will generally consist of a web based modelling tool, which also provides feedback on the current state of the system, and a set of background processes that perform the verification and feed changes back to the web interface.

All user interaction will be web based, due to the ease in distributing and updating the system and the lack of an installation step. Latest browsers with HTML5 and CSS3 support will be targeted as these are common amongst IT professionals. More details on the chosen system architecture are presented in section~\ref{section:architecture}

\textbf{Scope Limitations:} Due to the significant differences between Windows and UNIX based servers, including compatibility of tools, remote command execution strategies and scriptability, I have decided to target only UNIX based (specifically Linux) servers initially. This decision is based upon the dominance of UNIX within the server market, at 66.8\% in a 2013 study~\cite{UnixMarketShare}.

\chapter{Background Research \& Literature Review}

In this chapter, we discuss the increase in infrastructure and software complexity, and how the industry has adapted to deal with it. We also review some literature from related research fields and evaluate the effectiveness of the proposed solutions.

\section{Managing Software Complexity}

The problem of increasing software complexity is a well publicised and researched phenomenon. Software that is complex takes longer to develop, and is harder to maintain. As it affects the value proposition to the user so directly, it is often the dominant factor in determining the feasibility of implementing a given system.

The rise in software complexity together with ever changing business demands during its development has given rise to a number of popular methodologies or techniques in research and industry, such as Model Driven Development~\cite{MDD}, Extreme Programming~\cite{BeckXP} and the Agile movement~\cite{AgileManifesto}.

\subsection{Modelling and Model Driven Software Engineering (MDSE)}

Modelling of complex software has been used for a number of years as a tool of communication between developers, project managers and stakeholders. A number of tools can be used to produce clear diagrams of a system, but models provide a level of semantic meaning that allows them to be practically used to derive or verify an implementation. The Unified Modelling Language is considered the de-facto set of graphic notation techniques to create visual models of object-oriented software-intensive systems~\cite{UMLDefinition}.

\paragraph{Diagrams vs.\ Models}
A drawing tool can only be considered a modelling tool only if the tool `understands' the drawings~\cite[p.~11]{ModelDrivenDevelopment}. This has a number of benefits: 
A modelling tool guarantees a minimum level of semantic meaning and model quality, because they grant alignment to some sort of meta-model~\cite[p.~12]{ModelDrivenDevelopment}.
The model can often be used to derive an implementation or perform checks/validation to ensure that a given implementation satisfies the model. 

\paragraph{Modelling Techniques}
Models can be built textually as well as graphically. Textual models are commonly expressed using Domain Specific Languages (DSL’s). A DSL is defined as ``a computer programming language of limited expressiveness focused on a particular domain"~\cite{FowlerDSL}. Textual models can still sometimes be used with graphical tooling to provide a visualisation of the expressed model.

\paragraph{Adoption of Modelling and MDSE}

Studies into increased quality, maintainability and developer productivity when using MDSE techniques can be found commonly in academia but are difficult to find in industry. In a study into Industrial Adoption of Model Driven Development, it was stated that "interviewees emphasised tool immaturity, complexity and lack of usability as major barriers to adoption"~\cite{IndustryMDSE}.

\pagebreak
\subsection{Unit and Integration Testing}

The XP and Agile movements focus on improving software quality and increased responsiveness to changing customer requirements~\cite{WikiXP}. These are both process management and technological issues. XP focuses mostly on the technical aspect, there is a very strong focus on testing, and Test Driven Development (TDD) in particular which helps to realise these goals.

There are many types of testing, but they can generally be grouped into three main categories:

\begin{description}
  \item[Unit Testing] The main role of unit testing is as a design tool, developers can write unit tests to specify the intended behaviour of a component and then implement it to match. They provide fast and specific feedback on the correctness of implementation.
  \item[Integration Testing] verification that each separate component of the system functions correctly when it has been integrated with other components. This could range from testing the composition of two small components or an entire order processing pipeline. They are used to improve confidence in overall system correctness, and they are often use to guard against regressions in functionality over time.
\end{description}

Testing is often used as a means of managing complexity, especially as systems become larger. A suite of tests is often used as a means to provide quick feedback when changes break other areas of the system. We can see many similarities between the complexity of software and the complexity of its underlying infrastructure, therefore these techniques could have value in this area too.

\pagebreak
\section{Managing Infrastructure Complexity}

There is clearly a large amount of research and analysis into managing software complexity, in this section, we will review the techniques used to manage the infrastructure side of this complexity.

\subsection{General Modelling Tools}

A number of modelling standards are extremely flexible and allow the expression of arbitrary systems. One example of this is the Common Information Model (CIM) standard~\cite{CIM}. There are many IDE’s and tools that have the ability to construct CIM compliant models, such as the Eclipse Modelling Framework~\cite{EMF}.

It is relatively difficult to find practical examples of infrastructure modelling in CIM, making it difficult to get started with unless the user already has experience with MDSE techniques.

Such generic modelling tools are useful for communicating ideas or states of a system, but it could be argued that they lack pragmatic value. They are generally not  executable like their software modelling counterparts, which limits their value significantly.

\subsection{Infrastructure Modelling Tools \& Configuration Management}

Modern configuration management tools can be considered a type of infrastructure modelling tool. Most of these tools such as Puppet~\cite{Puppet}, or Chef\cite{Chef} use a Domain Specific Language to declaratively define hosts, their respective software stack, and install and manage their respective services.

Configuration management tools are especially interesting due to their mass adoption in recent years within the Agile software development community, as well as at enterprises such as Facebook and Amazon~\cite{Chef}.

The real value of these DSL's is that they are directly executable.  They can be executed to drive infrastructure changes and ensure convergence with the model. Figure~\ref{fig:PuppetDSLSnippet} provides an example snippet of a Puppet recipe, declaratively specifying details of the SSH (secure shell) service that will be made available on a specific host.

\begin{listing}[h]
\begin{minted}[fontfamily=fi4]{ruby}
class sshd {
    package { 'openssh-server':
        ensure => latest
    }
    service { 'ssh':
        subscribe => File[sshdconfig],
        require   => Package['openssh-server'],
    }
    file { 'sshdconfig': 
        name    => '/etc/ssh/sshd_config',
        owner   => root,
        group   => root,
        mode    => 644,
        source  => 'puppet:///sshd/sshd_config',
        require => Package['openssh-server'],
    }
}
\end{minted}
\caption{A puppet recipe snippet, declaratively configuring the SSH service on a host. The execution of this description is idempotent, so can be repeated safely to ensure convergence.}
\label{fig:PuppetDSLSnippet}
\end{listing}

As part of the runtime execution, each service’s conformance to this set of properties is verified and if there is any divergence, the tool will orchestrate a set of imperative actions, for example installing the service or altering its configuration file and then restarting it.

\clearpage
\subsection{Existing Monitoring Tools}

I have extensively researched existing monitoring tools ranging from very simple ping checks to complete enterprise solutions and evaluated their effectiveness. In the interest of brevity, I have included a small sample of popular monitoring tools I have evaluated that give an indication of the cross-section of the market:

\subsubsection{Nagios}

Nagios is an open source infrastructure and application monitoring framework. Often considered the `industry standard', it provides a vast array of features, from execution of fine-grained protocol specific tests to complex capacity planning forecasts. It is also very extensible, which is perhaps the largest reason for its success within the industry.

Nagios is in active use by over 250,000 organisations worldwide, including Amazon, AT\&T and JP Morgan Chase~\cite{NagiosUsage}. It supports a number of common monitoring architectures, including both agent based and agent-less (based on the Simple Network Management Protocol) that means it can run efficiently up to a very large number of hosts~\cite{NagiosArchitectures}.

Anecdotal feedback about Nagios typically suggests that it is a mature and flexible solution for basic monitoring of large numbers of hosts, but severely lacks some higher level concepts such as integration tests of system components. It is also considered extremely time consuming and counter-intuitive to set up and manage, which perhaps explains its lack of adoption within smaller businesses/startups.

\subsubsection{StatsD, CollectD, Graphite}

This collection of tools all work together in providing near real-time metrics tracking and graphing, they are used heavily at vastly successful startups such as 37signals, GitHub and Shopify. Metrics can be collected from the infrastructure and application layers. 
CollectD is a system utility written in C that periodically makes system calls to extract performance fundamentals such as CPU, memory and network usage. 

Application level metrics can be collected using StatsD. Applications can send metrics to a local StatsD instance via UDP, which adds very little performance overhead. This means that it is possible to heavily instrument the critical paths of an application except in extremely performance sensitive scenarios. Both of these tools aggregate this data and flush it to a backend Graphite instance at configurable intervals. Graphite provides extremely flexible data visualisation techniques, but requires some up front investment in time to produce meaningful results.

This collection of tools provides an extremely flexible and performant source of aggregate data that can be used to diagnose reliability and performance problems effectively. Its disadvantages however include a lack of any alerting mechanism, a number of independent components that are both non-trivial to deploy and difficult to monitor themselves, and a difficulty in actually interpreting aggregate data to draw conclusions without deep insight into both the application and the infrastructure it’s relying on.

\subsubsection{New Relic - APM} 
New Relic is a popular subscription-based, hosted Application Performance Monitoring (APM) service. It provides integration with popular languages, frameworks and platforms such as the Java Virtual Machine (JVM), Ruby/Ruby on Rails and Python with the Django web framework.

New Relic is extremely easy to integrate with existing web applications as it provides environment specific tooling such as Ruby gems or Python packages. It reports on fine-grained application metrics such as slow SQL queries, poor response times and browser based monitoring metrics with a snippet of JavaScript.

Whilst New Relic is extremely simple to set up and provides a fairly deep insight into application performance, it is a relatively expensive service beyond a trivial number of hosts, treats all hosts as independent nodes rather than interconnected services with dependencies, and lacks the extensibility of the open source solutions when it comes to monitoring other non-supported parts of the stack.

\subsubsection{Summary}
 A vast array of monitoring tools already exist, many of which are mature solutions. However, they generally require a large investment of time and do not help with visualising or managing infrastructure complexity. They often also require deep insight into the application/service being monitored, which is not always the case when developers and operations teams are separate.

An ideal solution would be an open source tool, incorporating the ease of setup of a service like New Relic, with the flexibility and alerting of Nagios and Graphite. It would also provide a holistic view of the system and its dependencies so it is easy to visually identify problems and their effects without deep insight into the application architecture.


\chapter{Requirements Specification}

A set of functional and non-functional requirements will be established from user stories based on my own personal experiences developing software in industry, and conversations with the following typical target users of the system:

\begin{itemize}
  \item A Technical Architect at Red Badger Consulting
  \item A System Administrator at HCL
\end{itemize}

\subsection{Functional Requirements - User Stories}
\begin{enumerate}
  \item As a developer, I would like a simple and expressive way to document my application's infrastructure requirements, so I can effectively communicate them to other developers and system administrators.
  \item As a developer, I have the ability to execute my documented infrastructure specifications to automatically validate the correctness of the system under test.
  \item As a developer, I can express arbitrary properties that the system under test must satisfy, while making use of open source libraries. So I can quickly produce accurate and practically useful specifications.
  \item As a system administrator, I can see a clear overview of the system, detailing its architecture and dependencies, so I can understand and effectively support it.
  \item As a developer / system administrator, I would like to verify the state of the system under test continuously, so I can quickly identify and resolve problems when they occur and make the system more amenable to change.
  \item As a developer / system administrator, I would like to capture changes within the system as they occur, such as code deployments or re-configuration, so I can relate these events to issues that may occur, making root cause analysis easier.
  \item As a developer / system administrator, I would like to be notified of changes in the state of the system via email and SMS, so I can respond to incidents quickly.
  \item As a system administrator, I would like to easily create specifications for applications I have already deployed.
  \item As a developer / system administrator, I would like to measure the reliability of a service and its components over time.
\end{enumerate}

\pagebreak
\subsection{Non-Functional Requirements}
\begin{enumerate}
  \item Reliability - The system and all of its components should provide a reliable platform for monitoring live systems.
  \item Usability - It should be easy to start using the system and understand how it fits within the users existing organisation. Understanding of the implementation details should not be necessary for basic use cases.
  \item Performance
    \begin{enumerate}
      \item The system should efficiently validate system correctness.
      \item Alerts or warnings should be delivered in a reliable and timely fashion.
      \item Validation of the system should scale above a trivial number of hosts. (Over 20)
      \item The impact of validating correctness on the system under test should be minimal.\ i.e. no significant degradation in performance of the monitored applications or resource exhaustion of the hosts under test.
    \end{enumerate}
  \item Security - The system is intended for use in production environments, therefore it should not unnecessarily expose the target system to threats such as the following:
    \begin{enumerate}
      \item Open network ports
      \item Insecure transmission of sensitive data
      \item Remote code execution
    \end{enumerate}
  \item Maintainability - The system should be maintainable so that new features are easier to implement and the implementation is easier to understand for any open source contributors.
    \begin{enumerate}
      \item Separation of Concerns - The system should be decomposed into a number of separate components which can be built and tested in isolation. They may use different languages and frameworks as appropriate for the specific task.
      \item Architectural Patterns - The system should make good use of appropriate design patterns and architectural techniques provided in the implementation language, such as encapsulation and composition in an object orientated language.
      \item Unit Testing - Wherever possible, the code should be tested by a suite of unit tests that achieve a high coverage of the total codebase.
    \end{enumerate}
  \item Interoperability - The system should integrate with existing systems in order to provide maximum flexibility/utility to the users. It should therefore:
    \begin{enumerate}
      \item Provide a simple interface such as a HTTP API to read and write data
      \item Expose data in a standard format such as XML or JSON
    \end{enumerate}
\end{enumerate}

\chapter{Design and Implementation}

\section{Design Objectives}

\paragraph{Speed/Ease of Setup} Many comments from developers detailed the fact that most monitoring and modelling tools were too time consuming to set up, and clients did not see demonstrable value in investing time upfront in this process before services were launched and issues actually occurred. 

A tool that provides a simple installation process, follows ‘convention over configuration’ and provides a degree of auto-detection of installed services would be extremely valuable and increase chances of adoption significantly.

\paragraph{Simplicity and Pragmatism} Often the need for extensive application and infrastructure monitoring is not recognised until after the application has been deployed into a production environment. Introducing monitoring at an earlier stage, e.g.. in a staging environment, can highlight issues of reliability and performance before they are experienced in production. Monitoring key domain specific metrics can also aid in reasoning about the runtime behaviour of the system. With these benefits in mind, the tool should be simple enough to introduce early in the development process with minimal effort on the part of the developer, it should provide pragmatic value from the start, and offer the flexibility to continue to deliver value as the system grows in complexity.

\paragraph{Embracing Change}

Maintaining agility while providing a suitable quality of service is challenging. Continuous Integration and Delivery are fundamental principles of the XP (Extreme Programming) movement and discuss solutions to this problem at great length, the most cited benefit is the reduction of risk. Small amounts of change applied frequently are easier to reason about and test, and of course provide the end user with value quicker. In conjunction with version control systems, CI/CD can be used to effectively identify the root cause of problems soon after they occur. The application of CI/CD is widespread within the industry, and a whole suite of tools such as Continuous Integration servers, automated code deployment tools and configuration management utilities all aid to facilitate the practice.

The monitoring tool should fit within an agile workflow, where system architectures change frequently. It could even be used to verify that system reconfiguration was successful, as a form of test driven infrastructure change. The tool should provide the means to track changes such as code deployments and re-configuration and provide an interface to visualise this, similar to viewing changes in code under source control.

\paragraph{Dynamic Infrastructure}

The nature of modern infrastructure is no longer static. Cloud computing and virtualization provide the means to scale different parts of the system independently and automatically. For example, Amazon AWS provides `auto-scaling groups' that create new instances of an application according to pre-defined load thresholds. Capturing and visualising these changes is something that existing monitoring tools struggle with, as they are aimed at more traditional, static infrastructure. The tool should provide the means for tracking these changes.

\paragraph{Flexibility}

It is imperative that monitoring tools do not just focus on common areas that are easy to measure or verify. The monitoring needs of an application could be highly domain specific, and overly simplistic verification can cause misleading results. The tool must provide the means to encode and verify arbitrary properties of the system at varying granularity. It should do this in the same way as a good unit testing framework does, with the full capability of a programming language, leveraging a vast array of existing open source libraries.

\paragraph{Streaming Metrics} The vast majority of monitoring tools are based upon a polling mechanism. Configured checks are performed from a centralised server at pre-defined intervals. Reversing this mechanism so that servers stream their own metrics back to the monitoring server addresses a number of issues with polling such as the following:

\begin{itemize}
  \item Firewall Reconfiguration - Typically firewalls do not allow inbound connections except where explicitly defined by a rule. This means that monitoring tools will not work until rules are explicitly added for them. Outbound connections usually do not suffer from this constraint, and therefore do not require reconfiguration of existing infrastructure.
  \item Poor Scalability - Performing checks from a single server does not scale to a large number of hosts, typically the solution for this is to increase polling intervals to reduce load. Streaming places the overhead on each server being monitored, where it can easily be tolerated. The management server should easily be able to process a stream of results from a very large number of hosts, and where this becomes less feasible, this could be load balanced.
  \item Polling Latency - If checks are scheduled infrequently, there is an artificial time delay in receiving notifications. Not only does this mean that issues are not identified as quickly, but the slow feedback can be confusing when attempts are made to fix the underlying problem.
\end{itemize}

\paragraph{Simple Install Process} The install process is a one-line bash command that downloads and executes the bootstrap shell script, that in turn detects system architecture and downloads the appropriate binary, copying it into the system path and configuring it to run as a service.

When the binary is first executed, it performs node registration by making a PUT request to the monitoring tool’s REST API with the hostname of the server, and supplementary meta-data.

\textbf{Example Install Command:}
\begin{minted}[fontfamily=fi4]{bash}
  curl -s https://management-server/agent/install | sh
\end{minted}

\paragraph{Automatic Service Detection} The tool builds in a basic service detection process that is executed during node registration and at rare periodic intervals after this point.
 It detects the presence of known services such as Apache2, Nginx or MySQL within the currently executing process table, these services are defined in a simple component definition list bundled with the agent software, making it easy to expand in future. It then takes further steps to extract version numbers from installed services and includes this information in the node meta-data.

There are a number of issues that prevent this process from being accurate in every scenario, but in practice it is a flexible solution that can detect most services accurately enough to be a useful time saving feature.

\paragraph{Hosted vs. On-Premise Solutions} The system should be built and packaged so that it could run as a hosted service (typically more suitable for startups, and easier to experiment with) and as an on-premise solution where there are concerns over data security or scalability. This can be achieved by using a configuration management tool for setup and deployment, which also has the capability to produce, as build artifacts, pre-built system images such as VMWare packaged appliances or Amazon Machine Images (AMI's).

\paragraph{Security} The installation of any networked monitoring tool increases the attack surface-area on production systems that often host important services and contain sensitive user data. In order to partially mitigate this concern, communication between the each node and the monitoring system will be encrypted, and both the server and client must verify each other’s respective identities to ensure that no Man in the Middle (MiTM) or general spoofing/poisoning attacks can become effective.

Public-Key cryptography can generally provide this functionality, if the agent installation is performed via HTTPS, the identity of the management server can be verified and a public key can be exchanged from this point forwards in all communication. In addition, communication between the nodes after initial setup is uni-directional, for the purposes of reporting metrics, so the monitoring agent does not need to listen on any network interfaces where it could be compromised.

This strategy needs to be carefully evaluated and tested, but should not present a major risk to the implementation. The difficulty will lie in expressing this capability to future users of the system so they can be confident in its security.

\pagebreak
\section{Domain Model and Concepts}

\begin{itemize}
  \item Service - An application that provides functionality to an end-user or another service over a network\. e.g. a website or API.
  \item Component - Part of a service that performs a specific function, e.g.\ an application server or a load balancer.
  \item Host - A physical, virtual or otherwise isolated container responsible for running some or all of the components of a service.
  \item Metric - A measure of a characteristic at a given point in time, e.g.\ CPU load or average request latency. Typically quantitative in nature.
  \item Check - Validation that an arbitrary property holds at a given point in time, e.g.\ A specific service is listening on port 80. Typically qualitative in nature.
  \item Event - A discrete action performed upon the system, e.g.\ a code deployment or service re-configuration. Events can be correlated with metrics.
  \item Incident - A specific type of event that is automatically triggered by the failure of one or more checks, incidents correlate recent events with failed checks and can be related to previous incidents of a similar nature to aid in root-cause analysis.
  \item Context - Additional data that aids in root-cause analysis, e.g.\ log files from a malfunctioning service.
\end{itemize}

\pagebreak
\section{System Architecture}
\label{section:architecture}

\subsection{Core Management API}

The application core is a REST API written with the Ruby on Rails framework. It consists of a relational or graph database containing all entities such as hosts, services and their respective dependencies. These entities are exposed via the REST API to other parts of the application. The application core provides the means for registering new hosts and generating appropriate agent configurations.

\subsection{Streaming Metrics/Events Service}

The streaming service manages the collection of test results, metrics and other events such as code deployments via event streams over TCP or UDP. Streams reporting errors or anomalous metrics will trigger status changes in the management API, as will streams that stop receiving messages, as this may indicate failure in the monitoring framework itself.

Due to its highly parallel and asynchronous nature, this service is written in
Clojure, as the functional programming style and asynchronous primitives within
the language make it an ideal candidate for this type of processing. The Riemann
~\cite{Riemann} event aggregation and stream processing library is used for stream buffering and filtering.

\subsection{Web UI}

The web interface is the only user facing component of the system. It is a single page application written in JavaScript, and uses the Facebook React.js library for event handling and data binding, providing structure and testability to the frontend code. Where interactive visualisations are used, the D3.js library is used to draw these graphics as Scalable Vector Graphics (SVG).

The Web UI will subscribe to a WebSocket connection to listen for changes in the system, this will ensure that any changes in the system are immediately visible to the end user, a feature that many other monitoring systems do not provide.

\subsection{Monitoring Agent}

The server agent is a lightweight component written in Ruby and is installed on each server. The installation process is initiated by a shell script, downloaded and executed from an installation endpoint on the Core API. The agent is responsible for the collection of metrics provided by the operating system and installed applications, as specified in the configuration file, this configuration file is synchronised with the core API at pre-defined intervals.

The monitoring agent will manage forwarding of metrics to the correct endpoint, collection will be delegated to any number of bundled plugins, making data collection highly flexible.

\begin{landscape}
  \subsection{Monitoring Architecture Diagram}
  \includegraphics[scale=0.7]{architecture.pdf}
\end{landscape}

\pagebreak
\section{Domain-Specific Language}

Providing the flexibility needed to encode arbitrary checks within the monitoring tool is difficult to achieve without a very large amount of static configuration. The full expressive power of a real programming language, leveraging a vast array of open source libraries is perhaps a better way to approach this problem.

Due to its meta-programming features and flexibility in syntax, Ruby is somewhat uniquely amenable to creating domain specific languages within the language itself, so called `internal DSLs'.

The DSL is heavily inspired by a similar DSL of the RSpec unit testing framework
~\cite{RSpec}, RSpec is extremely popular within the Ruby community as it leads to highly-readable test cases. The similarities should provide a common medium for communicating the monitoring tool to those who have used RSpec or similar in the past.

\subsection{Meta-Programming}

Ruby features typically leveraged to create internal DSL's are among the following:

\begin{itemize}
  \item{Blocks} - A fairly common feature of most languages today, Ruby supports `blocks' as an implementation of first-class functions. First-class functions can be used as values, and therefore passed around and evaluated at will.

  \item{instance\_eval} - This function executes a given block in the context of the receiving object. This means that lots of typical structural techniques such as defining classes and methods can be hidden as part of the implementation, and relied upon during execution. An example of this is included below:

  \begin{minted}[fontfamily=fi4]{ruby}
  # Internal DSL implementation
  class Service
    def description(value)
      @description = value
    end
  end

  def service(&block)
    Service.new.instance_eval(&block)
  end

  # DSL interface
  service do
    description "An example service"
  end
  \end{minted}
  \label{fig:MetaProgrammingSnippet}

  \item{method\_missing} - As a dynamically typed language, function calls in Ruby are dynamically dispatched depending on the type of an object. The mechanism for dynamic dispatch also includes a fallback feature used when there are no matching methods with the given name. This means that DSL's can be expressed that pattern match on method names or provide transparent proxies to other objects. Both of these features can be used extensively to create concise, readable DSL's.
\end{itemize}

\subsection{Features and Implementation}

The DSL supports the construction of \textbf{services} and \textbf{components}, they can also be nested, making the hierarchy easy to understand:

\begin{minted}[fontfamily=fi4]{ruby}
  service "Example Service" do
    description "A website"

    component "Nginx" do
      description "The web server"
    end
  end
\end{minted}
\label{fig:MetaProgrammingSnippet}
\pagebreak

The most important feature of the DSL is the ability to encode checks as Ruby blocks. A block contains one or more assertions that throw exceptions with descriptive error messages if they fail. Below is a simple example of this. The health check will be executed every 30 seconds by the monitoring agent. The monitoring agent will catch any exceptions and return a descriptive error message, which may be printed to the console or serialised and sent to the event stream.

\begin{minted}[fontfamily=fi4]{ruby}
  service "Example Service" do
    health(interval: 30) do
      running "example-service"
      listening 80
      success "http://example.com"
    end
  end
\end{minted}

Importantly, as the blocks executed are pure Ruby code that can throw exceptions, any arbitrary code that fits these criterion can be used within a block:

\begin{minted}[fontfamily=fi4]{ruby}
  service "Example Service" do
    health(interval: 30) do
      ssl_expiry_date < DateTime.now or fail "SSL certificate expired"

      require "redis" # External RubyGem
      Redis.new.ping == "PONG" or fail "Redis isn't responding"
    end
  end
\end{minted}

With these simple but powerful primitives, an accurate executable model of the system can be constructed and used for validation purposes. When constructing these service specifications, the agent software can execute all checks on demand so that they can be tested before deployment to a large number of hosts.

This code as configuration approach to monitoring provides a very similar experience to writing unit tests. It can be placed under source control within the existing project repository and therefore fit easily with an existing development workflow, this was a core design goal of the project.

\section{Monitoring Agent}

The monitoring agent is written in Ruby, and is packaged as a RubyGem. As the monitoring agent is responsible for executing the Domain Specific Language outlined in the previous section, it must be written in Ruby itself or package the Ruby runtime. Despite the fact that Ruby is not typically pre-installed on servers, it is very simple to install and requires no configuration. The installation procedure manages the installation of Ruby if necessary.

\subsection{Installation}

Ease of installation was a core principle of this system, getting started is a one line bash command:

\begin{minted}[fontfamily=fi4]{bash}
  curl -s https://management-server/agent/install | sh
\end{minted}

Importantly, the installation of the management software occurs over HTTPS, this provides assurances of host identity. The management server must present a valid certificate, mitigating concerns against man-in-the-middle (MiTM) attacks. If this was not the case, it would create an attack vector whereby arbitrary code could be executed on the deployed servers at the point of installation.

The installation script performs the following steps:

\begin{enumerate}
  \item{Detection of Operating System} - The tool initially supports Linux and Mac OS X only, it will distinguish between these operating systems or abort the installation if running on a different platform.
  \item{Detection or Installation of Ruby} - If Ruby 2.0.1 is not installed, the script will use the standard package manager of the distribution to install it.
  \item{Download of Agent Software} - The source code will be obtained directly from GitHub, dependencies will be resolved and installed.
  \item{Host Registration} - Optionally, the host can be registered with the management server, the hostname and IP address as well as an optional service name and environment will be passed to the management server. This is appropriate on servers but not development workstations.
\end{enumerate}

\subsection{Commands and Usage}

\begin{description}
  \item[Check] - Run configured health checks on demand. This command is typically used on developer workstations as they iteratively build a service specification using the DSL\. It provides fast feedback similar to running a suite of unit tests and can be used to quickly validate assumptions, before pushing the specification to the management server.
  \item[Discover] - Creates a service specification from detected system services. This feature can be used primarily to reverse-engineer existing infrastructure setups into a service specification. More details about the discovery mechanism are provided in the next subsection.
  \item[Graph] - Render a graph of the current service - This can be primarily used on the developer workstation, it will use the GraphViz library to render a graph of the system and its dependencies image or PDF format. This can be used on its own as a form of documentation, but is typically intended to help visualise the service specification and its dependencies to ensure its correctness.
  \item[Push] - Pushes the specified service to a management server - This command will parse the DSL and serialise each object to JSON, it is then sent as a PUT request to the management API\. Changes to the service specification take effect immediately and update any connected users in near real-time.
  \item[Setup] - Register this host with the management server - This is used on the servers running the system under test, it will associate a host with a defined service, listing it in the Web UI and accepting events and check data from it.
  \item[Start] - Starts a monitoring daemon process, which runs in the background and executes checks according to their defined intervals. It will report all data back to the management server's event stream endpoint. Any errors occurring during this process will be logged to a file for later analysis.
  \item[Stop] - Stop monitoring the specified service.
  \item[Restart] - Restart monitoring of the specified service.
  \item[Help] - Describes the function of all available commands.
\end{description}

\subsection{Service Discovery}

Reverse-engineering a system configuration is a non-trivial task. As this is not the primary intention of the tool, a simple approach is used to detect commonly installed services and provide some common default health checks for them. It relies upon standard UNIX commands such as `ps', `lsof' and `netstat' to detect running services.

The algorithm reads from a list of component definitions similar to the following:

\begin{minted}[fontfamily=fi4]{yaml}
database:
  Postgres:
    process: postgres
    description: relational database
    ports: 5432
    version: postgres --version | cut -d ' ' -f3
  MySQL:
    process: mysqld
    description: relational database
    ports: 3306
    version: mysql --version
\end{minted}

It then gets the table of currently executing processes, via the UNIX command `ps aux' and parses the output. It then matches this list with the processes in the service definitions list, and verifies the listening port via `lsof :port'. When a process has been detected, the tool will execute the specified `version' command to extract the current version number of the service.

The discovery process will output a service specification similar to the following:

\begin{minted}[fontfamily=fi4]{ruby}
service "<Service Name>" do
  component "Postgres" do
    description "relational database"
    type :database
    version "9.3.3"

    health do
      running "postgres"
      listening 5432
    end
  end
  component "ElasticSearch" do
    description "open source search and analytics engine"
    type :database
    version "1.0.1"

    health do
      running "elasticsearch"
      listening 9200
    end
  end
end
\end{minted}

This service specification not only lists the running services, but provides a suggested set of health checks for them, as appropriate. This can help new users of the tool become familiar with the syntax and features of the DSL.

\pagebreak
\section{Streaming Event Service}

The streaming event service is based on the open source project
Riemann\footnote{See http://riemann.io/concepts for an introduction to
Riemann.}. Riemann is a simple and scalable event processing framework written in Clojure\footnote{See http://clojure.org for a full description of Clojure}. Riemann simply provides a powerful set of primitive operations to filter, throttle and detect changes in streams of data. Riemann provides a configuration file, itself written in Clojure, which is used to specify the desired behaviour.

Clojure is especially well suited to a problem like this as it has excellent support for asynchronous execution, it is hosted on top of the Java Virtual Machine and therefore benefits from a mature ecosystem of tooling and performant networking libraries. It is a functional language, and therefore makes transforming and filtering event data very simple.

Events are sent from the monitoring agent installed on each host via a TCP
socket. They use the `Protocol Buffers'\footnote{See https://developers.google.com/protocol-buffers/docs/overview for an introduction} binary format developed at Google to efficiently transport the data.

\subsection{Event Schema \& Routing}

A typical event payload can be arbitrarily defined, but is typically structured as follows:

\begin{minted}[fontfamily=fi4]{yaml}
  host: "example-production-1"
  service: "example-service"
  component: "nginx"
  description: "Error: Process nginx not listening on port 80"
  state: "error"
  metric: 0
  type: "component"
  tags: ['check']
  notify_endpoint: "/example-service/components/nginx"
  ttl: 15
\end{minted}

The streaming event service provides one endpoint to send all types of event data, this includes results from status checks, application performance metrics and log file entries. These types of event can all be processed and routed to a number of locations. This agnosticism makes it easy to adapt the monitoring architecture in future, hosts can be configured to send data to a single endpoint, which in turn can route the events to their appropriate location.

Another important principle of Riemann is the notion of a time-to-live (TTL) sent with each event. After the number of seconds specified in the TTL field, specified by the sender, Riemann will change the event type to `expired'. This means that if the agent software stops sending events due to a loss of connectivity or a software defect, some error handling logic can be executed, such as updating the component's state to `unknown'. Without this feature, the streaming mechanism would not match the polling based mechanism for robustness.

Checks and performance data are checked for state changes or anomalies before being forwarded to Graphite, a time series database for long term storage and queries. The introduction of Graphite integration means that the framework provides a gradual roadmap to scaling up monitoring granularity and metrics collection. More complex metrics such as median and percentile based performance data can be queried from Graphite and used for anomaly detection.

\subsection{Interpreting Check Results}

\begin{enumerate}
  \item The main event stream is filtered down to events with the tag `check'.
  \item The `metric' field of the event is forwarded to Graphite, under an appropriate path, e.g. `example-service.nginx.health'. In the case of health checks, the value is simply 0 or 1, signifying failure or success respectively.
  \item If the most recent event has a different `state' field to the previous event of the same type, queue a notify task. This could happen when a check transitions state due to an error, or where another check packet was not received within the expected TTL interval.

    The number of events matching these criteria is very low relative to the total number of events, and therefore most events never reach the management API\. This is a deliberate design decision, as the management API is significantly less capable of handling a high frequency of updates.
  \item The notify task sends a HTTP PATCH request to the management API at the endpoint specified in `notify\_endpoint' signifying a partial update with the new component status. If the status is `error', the request will also contain a description of the error that occurred. This error is often specific enough to pinpoint exact problems within failed components, but this depends on the granularity of the checks involved.
\end{enumerate}

\pagebreak
\begin{minted}[fontfamily=fi4]{clojure}
; Forward all events to graphite, and check for state changes
(streams
  (tagged "check"
    forward-graphite
    (by [:host :service :component :type]
            (changed :state notify-async))))

; Adds events to an async queue
(def notify-async (async-queue! :notify {:queue-size 1000} notify))

; Send a PATCH request to the monitoring API updating the status
(defn notify [event]
  (client/patch (notify-url (:notify_endpoint event))
     {
       :body (json/generate-string {
         (:type event) {:status (:state event) }
     })
     :content-type :json
     :accept :json
   }))
\end{minted}

\pagebreak
\section{Management API}

The Management API is provided as the interface to the core application logic and domain model. It is responsible for storage and retrieval of all entities such as hosts, services and their respective dependencies. These entities are exposed via the REST API to other parts of the application. The API accepts and responds with JSON\footnote{http://www.json.org/js.html} serialized data. The API is simple and discoverable, encouraging integration with other systems.

\subsection{Ruby on Rails}

Ruby on Rails is an extremely popular web framework written in Ruby. It provides a full suite of functionality from database access to view rendering. It has an extremely active community, over 3,400 contributors\cite{RailsContrib} and has been downloaded nearly 34 million times according to RubyGems.org~\cite{RailsPopularity}. It embraces common best practice patterns such as Model-View-Controller (MVC) and the ActiveRecord pattern~\cite{ActiveRecord}. Ruby on Rails was chosen for this project for the above reasons, as well as the excellent support for testing libraries such as RSpec.

Whilst Rails is extremely popular, it is typically used for websites with a HTML interface, not pure REST API's. Many of the convenient features of Rails are not necessary or useful without a view layer. As Rails is a modular framework, a number of Rails core contributors released a subset of Rails functionality useful for API's. This project, known simply as `rails-api' is an excellent fit for the management API.

\subsubsection{ActiveRecord}

The ActiveRecord\cite{ActiveRecord} pattern, and its Ruby implementation with the same name is the default object-relational mapper/persistence layer within Ruby on Rails.

The pattern provides tight coupling between domain objects and the respective database tables that persist the data. ActiveRecord is a very convenient and low friction way of adding persistance to the application, but it has a few a disadvantages. It's main disadvantage is that it is very tightly coupled to the domain objects, this makes them harder to write isolated unit tests for, without extensive mocking of the database. There are, however, good solutions for the burden and fragility of database mocking within Ruby gems such as NullDB~\cite{NullDB}.

Another popular alternative to the ActiveRecord pattern is DataMapper\cite{DataMapper}, this pattern involves manually mapping objects to their respective database rows and vice-versa, it is useful in applications with more complex data models, but ActiveRecord was chosen for the increased productivity and implicit framework support.

\subsection{REST}

The API generally conforms to the Representational State Transfer (REST) architectural style. Each domain entity is exposed as a REST resource, these resources provide mappings from HTTP verbs to controller actions similar to the following.

\subsubsection{Semantic use of HTTP verbs}

\begin{table}[h]
  \centering
\begin{tabular}{r|l|l|}
  GET & /services & Retrieve all services \\
  POST & /services & Add a new service \\
  PUT & /services/:service & Add or overwrite a service \\
  PATCH & /services/:service & Partially update the service \\
  DELETE & /services/:service & Delete the service \\
\end{tabular}
\end{table}

Each of these endpoints will return semantic and appropriate HTTP status codes as part of their response. For example, if a service fails validation, it will be rejected with a `422 - Unprocessable Entity' response, whereas an Internal Server Error would return a HTTP 500 status code.

The adherance to these standards should mean that the API is simple and familiar to other developers, making it easy to integrate with.

\subsubsection{Content Negotiation}

The API will typically be used via JSON serialized requests and responses, but other formats such as XML and YAML are also possible to use by default with standard Ruby on Rails features. The API supports the `Accept' and `Content-Type' HTTP headers, where the client can provide details of their desired request and response formats.

\subsection{Root}

The root of the API contains an index of available resources, this is a weak form of the Hypermedia as the `Engine of Application State (HATEOS)' unified interface, which states that API's should be discoverable without prior knowledge. The discoverability also theoretically means that URI's can change in future and not break integrations, although this benefit is not typically realised. The JSON response from the root endpoint looks similar to the following:

\begin{listing}[h]
\begin{minted}[fontfamily=fi4]{json}
  {
    "services": "https://management-server/services",
    "hosts": "https://management-server/hosts",
    "events": "https://management-server/events",
    "incidents": "https://management-server/incidents"
  }
\end{minted}
\end{listing}

\subsection{Services \& Components}

Services are the top-level domain concept, they contain some meta-data, reliability calculations, a status such as `error' or `ok' which can be updated via the streaming events service, and a set of components and hosts. The management API will keep track of each service, it's components and hosts, and trigger notifications whenever any of their properties change. This data is normally manipulated via the management agent DSL, which will make a PATCH request to each service when the `push' command is used, updating it if necessary.

\begin{listing}[h]
\begin{minted}[fontfamily=fi4]{json}
  {
    "name": "Example Service",
    "status": "ok",
    "description": "Intranet site for Example Company",
    "dependencies": [],
    "components": [],
    "url": "https://management-server/services/example-service",
    "graphite_path": "services.example-service",
    "mean_time_between_failure": 64800,
    "mean_time_to_recovery": 603,
    "hosts":[]
  }
\end{minted}
\end{listing}

Standard measures of system reliability are calculated at the service level, for example `Mean Time Between Failure" (MTBF) and `Mean Time To Recovery" (MTTR), these measures give a good indication of how often and how serious any issues are. These measures can be supplemented at the UI level with other measures from Graphite, such as service uptime/availability.

\begin{equation}
  \mbox{Mean Time Before Failure (MTBF)}= \frac{\sum{(\mbox{start of downtime} - \mbox{start of uptime})}}{\mbox{total failures}}
\end{equation}

\begin{equation}
  \mbox{Mean Time To Recovery (MTTR)}= \frac{\sum{(\mbox{end time of incident} - \mbox{start time of incident})}}{\mbox{total incidents}}
\end{equation}

\begin{equation}
  \mbox{Service Availability}= \frac{\mbox{MTBF}}{\mbox{MTBF + MTTR}}
\end{equation}

\pagebreak
\subsection{Hosts}
Hosts are a simple structure, they are conceptually just an instance of the management agent running on a physical or virtual server. When the management software is installed, the user can specify an environment, e.g.\ development, staging or production. Hosts are added and removed based upon the `setup' and `remove' functions of the management agent.

\begin{listing}[h]
\begin{minted}[fontfamily=fi4]{json}
  {
    "hostname": "db-1.local",
    "ip": "192.168.1.100",
    "environment": "development",
    "service_id": 3,
    "status": "ok"
  }
\end{minted}
\end{listing}

\subsection{Events}
Events are intended to capture every possible change within the system, so that they can be used for root-cause analysis when issues occur. Keeping track of infrastructure changes is similar in nature to viewing a list of code changes over time in version control software, and can be very valuable in narrowing down the real cause of an issue.

Events originate from two sources, the application itself or an external integration. Internal events are raised as the result of failed checks, new host registrations etc. It is encouraged for developers to integrate their own deployment or configuration management processes into the events API so that those changes can also be tracked. For example, a code deployment could be tracked as follows:

\begin{listing}[h]
  POST request to /events:
\begin{minted}[fontfamily=fi4]{json}
  {
    "type": "deployment",
    "host": "rb-production-01",
    "service": "example-service",
    "component": "application",
    "url": "https://github.com/joestanton/example-service/commits/0c4f3dd",
    "branch": "master",
    "build": "0c4f3dd",
    "committer": "Joe Stanton"
  }
\end{minted}
\end{listing}

Events provide the context needed to identify the cause of an issue, and discuss that issue with the relevant collaborator on the project.

\pagebreak
\subsection{Incidents}

Incident detection is a layer of logic above events. It will detect events of a given type, e.g. a component check failure, and open an incident for the service involved. Any subsequent failures from the same service will be grouped under one incident. Incidents trigger Email and SMS notifications where configured, alerting the system administrators and developers to the problem. When all components of the service return to a normal state, the incident will automatically be marked as resolved.

\begin{listing}[h]
\begin{minted}[fontfamily=fi4]{json}
  {
    "name": "Incident #435",
    "status": "resolved",
    "created_at": "2014-04-01T16:57:44.898Z",
    "resolved_at": "2014-04-01T17:58:14.924Z",
    "resolved_by": "Joe Stanton",
    "root_cause": "The file descriptor limit was exceeded by Nginx.",
    "service": {
      "id": 3,
      "name": "Example Service",
    },
    "components": [ "Nginx" ],
    "hosts": [ "rb-production-1" ]
  }
\end{minted}
\end{listing}

Incidents are designed to group all of the required context such as component failures, recent events such as deployments, and past incidents of a similar nature. When an incident has been resolved, the system will trigger an email requesting that the investigator of the problem provides a short summary of the root cause of the issue. The API will also track statistics such as the time taken for recovery to occur, which is then used in reliability calculations at the service level. This information is then stored and can be used to help solve similar issues in future, especially where the same infrastructure problems keep recurring.

\subsection{Live Updates - Server Sent Events}

As well as the typical API resources previously discussed, a single endpoint is provided for clients to subscribe to a streem of changes. Where appropriate, all domain entities will be published to the changes stream as they are created, updated or deleted. Clients such as the Web UI can subscribe to these changes and update the user interface accordingly.

The changes feed is implemented as a Server Sent Events (SSE) stream, SSE is a HTML5 standard for uni-directional, long running HTTP responses. Alternatives to Server Sent Events are WebSockets and standard long polling techniques. SSE was chosen over WebSockets for the following reasons:

\begin{description}
  \item[Uni-Directional] There is only a need to send messages from the server to the client.
  \item[Pure HTTP] As a higher level, text-based protocol, HTTP incurs more overhead than TCP sockets (the level at which WebSockets operate), but is far more compatible with typical network infrastructure such as corporate web proxies etc. If WebSockets was used, it is likely that the TCP connection could be denied by proxies or firewalls.
  \item[Good Browser Compatibility] Although SSE is only natively supported in modern browsers, it is possible to effectively polyfill this functionality in older browsers using JavaScript. This is not required for this project, but could be a consideration if the tool increased in popularity.
\end{description}

Both SSE and WebSockets were deemed to be better implementations than a long-polling mechanism as they are more efficient, and require less custom logic on the receiving client.

\pagebreak
\subsection{Email and SMS Notifications}

The management API implements two methods of notification, Email and SMS. Both of these types of notification are intended to alert developers or system administrators of new incidents opened on services they are responsible for.

The email notifications are triggered when incidents open or are subsequently resolved. They provide an at-a-glance summary of the issue, together with a link to the Web UI for viewing in more detail. The emails are created via the ActionMailer functionality in Ruby on Rails and sent via the third party service SendGrid via SMTP. SendGrid ensures the reliable delivery of these emails.

\begin{figure}[h]
\includegraphics[scale=0.6]{email.png}
\caption{An example email notification, triggered when an incident has been detected.}
\end{figure}

SMS notifications can convey a more limited quantity of information than emails, but can be useful when the user in question does not have access to the internet. SMS messages are sent via the third party service Twilio. Twilio provide a REST API and can deliver messages to the provided phone numbers internationally.

\pagebreak
\section{Web UI}
\subsection{Implementation Choices}

\subsubsection{Single Page Application}

%The Web UI is implemented as a single page web application, written in JavaScript. Single page applications have recently increased significantly in popularity, due to the improved capabilities of modern browsers. A single page application differs from a traditional website in the following ways:

%\begin{description}
  %\item[Rendering]
    %Most web applications are executed exclusively on the server-side, this is an obvious choice as it provides many benefits such as: low-latency database access, the standard execution environment of a server under the developers control and effective security. With a single page application, the server side components are usually exposed via an API which takes advantage of these benefits, and the client-side JavaScript fetches data from this API via AJAX requests. When data is recieved, it is rendered into HTML elements by JavaScript executing in the browser.
  %\item[Interactivity and Responsiveness]
    %The user experience of a single page application can be significantly better than a traditional website, when used in an appropriate situation. Websites can feel much more like a desktop application, with multiple views, background data synchronization, data filtering etc. The main increase in responsiveness comes from eliminating jarring page reloads, thereby increasing percieved performance.
%\end{description}

%Abstraction and Maintainability

    %Single page application frameworks often apply standard architectural patterns such as Model-View-Controller. These patterns can help to raise the level of abstraction when creating highly interactive applications beyond manually binding event listeners to DOM elements in order to update and present data. Applications that are abstracted from the imperative nature of programming with the DOM (Document Object Model) can find benefits in a more delcarative approach. This can help improve testability, code reuse and can reduce the number of defects.

%\subsection{Features}

%The Web UI is the primary method of interaction for users of the system. It provides the following main features:

%\subsubsection{A Service Overview}

%The service overview provides the information necessary to understand the high-level architecture a service. It gives an overview of the service itself, with its description if specified, a number of reliability metrics and the current service status.

%All individual components and hosts are listed, together with their respective statuses. If there are any issues within the service, they will be highlighted and enhanced with extra detail to aid in the troubleshooting process.

%\subsubsection{Incident Tracking}

%As previously discussed, Incidents are a consolidated view of relevant context that users can use to identify the root cause of issues. Users will be directed to the incident page via notifications recieved via Email or SMS\. The interface surfaces relevant events occurring before or during an incident, as well as similar incidents that have occurred in the past.

%After an incident has been resolved, the user is encouraged to provide a brief note detailing the root cause, as well as any remedial action taken. This will be surfaced when other relevant incidents occur in future.

%\subsubsection{Event Logging}

%The monitoring system is designed to accomodate and track changes in underlying infrastructure. The event log provides a chronological view of these changes together with supplementary information provided via each integration. For example, a deployment event will normally include meta-data such as the hosts affected, the branch deployed as well as the specific build or version number if available. The interface provides the means to filter these events, so users can easily identify changes in relevant parts of the system if issues occur.

%* At a glance status of all monitored services and components.
%* Detailed error messages in the case of failed services, nodes or components.
%* Reliability metrics calculated and augmented with Graphite stats.
%* Incident tracking
%* Access to the full event log

Tech
* 'Single page' application - What does this mean?
* D3.js
* SSE/WebSockets

\subsection{React.js}
\subsection{Data Visualisation - D3.js}
\subsection{Live Updates}

\pagebreak
\section{Deployment \& Provisioning}

\pagebreak
\section{Tools and Methodologies}
\begin{description}
  \item [Version Control]\hfill \\
    Due to the size of the project and significant development effort involved, I will be using \textbf{Git} to version control my codebase. This will be hosted in \textbf{GitHub} for redundancy and issue tracking features. The project will be split into a number of independent services as outlined above, which will likely exist as separate code repositories.
  \item [Test Driven Development]\hfill \\
    It is of utmost importance to ensure that the codebase of the monitoring tool is as free from defects as possible. An unreliable monitoring tool eliminates all of the safety guarantees a stable tool would provide. Therefore I will be using Test Driven Development to write tests and relate them to features as I develop the solution.

    Where this is particularly difficult due to a large number of side effects or dependencies, I will use integration testing to provide some high-level guarantees and ensure that the system satisfies its requirements. A small amount of manual testing will also be required.
  \item [Vagrant and Amazon AWS]\hfill \\
    Where integration tests are required, I will use Vagrant\footnote{See
    https://docs.vagrantup.com/v2/why-vagrant/index.html - Benefits of using Vagrant}, a Ruby scripting tool that controls
    VirtualBox\footnote{See https://www.virtualbox.org/ for an overview of VirtualBox} and automates the provisioning and setup of Virtual Machines according to a specification.

    I can use these provisioned virtual machines to test cross-platform compatibility of the solution, and to simulate failures and verify the behaviour of the monitoring tool.

    For real world tests, Amazon AWS will be used to provision cloud based instances together with the API to alter the environment (such as security groups and hardware specifications) and initiate failures.
\end{description}

\pagebreak
% \chapter{Professional Issues}
%\chapter{Evaluation}
% DSL
% Streaming Performance
% \chapter{Conclusion & Future Work}
% \chapter{Appendices}

\printbibliography[title=References]
% \include{appendix}

\end{document}
